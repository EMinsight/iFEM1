{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "significant-services",
   "metadata": {},
   "source": [
    "Simple Iterative Methods\n",
    "=\n",
    "\n",
    "In this chapter we learn\n",
    "\n",
    "* The Richardson Iteration\n",
    "* The Gradient Method\n",
    "* Using Preconditioners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-toronto",
   "metadata": {},
   "source": [
    "We are given a linear system of equations\n",
    "\n",
    "$$\n",
    "A x = b.\n",
    "$$\n",
    "\n",
    "The matrix $A$ is so large such that direct elimination is not a good option. \n",
    "Although this section applies to linear systems in general, we think \n",
    "of equations arising from finite element discretization. Other numerical methods for partial differential equations lead to similar systems. Matrices from graph networks are another example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-rouge",
   "metadata": {},
   "source": [
    "The situation is advantageous if the matrix is\n",
    "* symmetric:  $$A = A^T$$\n",
    "* positive definite: $$x^T A x > 0 \\qquad \\forall \\, 0 \\neq x \\in R^n$$\n",
    "\n",
    "We call symmetric and positive definite matrices SPD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-turkish",
   "metadata": {},
   "source": [
    "We do not assume that the matrix is stored as a dense matrix, i.e. all $n^2$ entries are stored in a two dimensional array.\n",
    "\n",
    "We only assume that the matrix-vector product\n",
    "\n",
    "$$\n",
    "y := A * x\n",
    "$$\n",
    "\n",
    "is computationally available. \n",
    "\n",
    "Typically, the algorithmic complexity of the matrix-vector product is $O(n)$. \n",
    "\n",
    "Often one stores the matrix in a sparse matrix format, such as the CSR (Compressed Sparse Row) format. https://de.wikipedia.org/wiki/Compressed_Row_Storage https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    "\n",
    "\n",
    "More general, the matrix-vector product can be any linear operator. We can think of the sum or product of linear operators, multiplying with the inverse of the diagonal, or solving a triangular (sparse) system by forward/backward substitution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-baker",
   "metadata": {},
   "source": [
    "The Richardson Iteration\n",
    "---\n",
    "also called simple iteration is the fixed point iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-dining",
   "metadata": {},
   "source": [
    "$$\n",
    "x^{k+1} := x^k + \\alpha \\, (b - A x^k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-commander",
   "metadata": {},
   "source": [
    "with an arbitrary starting value $x^0 \\in R^n$, and a properly cosen damping paramter $\\alpha$.\n",
    "\n",
    "The solution $x^\\ast$ is a fixed point of the iteration (since $b - A x^\\ast = 0$).\n",
    "\n",
    "If we define the error as\n",
    "$$\n",
    "e^k = x^k - x^\\ast,\n",
    "$$\n",
    "\n",
    "the error propagation from one step to the next is\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "e^{k+1} & = & x^{k+1} - x^\\ast = x^k + \\alpha (b - A x^k) - x^\\ast \\\\\n",
    "& = & x^k - x^\\ast + \\alpha A (x^\\ast - x^k) = (I - \\alpha A) (x^k - x^\\ast)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "This means the new error is obtained from the old error by the error propagation matrix\n",
    "\n",
    "$$\n",
    "e^{k+1} = (I - \\alpha A) \\, e^k\n",
    "$$\n",
    "\n",
    "Two strategies to verify convergence are:\n",
    "\n",
    "* prove that the convergence radius \n",
    "\n",
    "$$\n",
    "\\rho(I - \\alpha A) := \\max_{\\lambda \\in \\sigma(I - \\alpha A)} |\\lambda| < 1\n",
    "$$\n",
    "\n",
    "* find some norm $\\| \\cdot \\|$ such that the matrix norm (=operator norm)\n",
    "\n",
    "$$\n",
    "\\| I - \\alpha A \\| := \\sup_{x \\in R^n} \\frac{ \\| (I - \\alpha A) x \\| }{ \\| x \\| } < 1\n",
    "$$\n",
    "\n",
    "The first one, $\\rho < 1$, only provides asymptotic convergence. This is easily proven if A is diagonizable, i.e. it features a full set of eigenvectors $z^j$ and eigenvalues $\\lambda_j$. Expand the initial error as\n",
    "$$\n",
    "e^0 = \\sum_j e^0_j z^j,\n",
    "$$\n",
    "then\n",
    "$$\n",
    "e^k = \\sum_j (1-\\alpha \\lambda_j)^k e^0_j z^j\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\| e^k \\| \\leq \\rho^k  \\sum_j | e^0_j | \\| z^j \\|\n",
    "$$\n",
    "This means $\\| e^k \\| \\leq C \\rho^k$, the the error does not have to decrease monotonically.\n",
    "\n",
    "However, if $\\| I - \\alpha A \\| < 1$, then\n",
    "$$\n",
    "\\| e^k \\| \\leq \\| I - \\alpha A \\| \\, \\| e^k \\|,\n",
    "$$\n",
    "and the error decreases in every interation step. Note that the matrix norm is the operator norm generated by the vector norm.\n",
    "\n",
    "Some facts:\n",
    "* If the norm $\\| \\cdot \\|$ is generated by an inner product $\\left< \\cdot, \\cdot \\right>$ (parallelogram identity), and $M$ is some self adjoint matrix with respect to this inner product, i.e.\n",
    "$$\n",
    "\\left< M x, y \\right>  = \\left< x, M y \\right>,\n",
    "$$\n",
    "  then $\\rho(M) = \\| M \\|$\n",
    "\n",
    "  If $\\left< \\cdot , \\cdot \\right>$ is the Euklidean inner product, then $M$ is self-adjoint exactly when $M$ is symmetric.\n",
    "  \n",
    "* Every operator operator norm is bounded by the spectral radius. There exists some norm such that the operator norm is arbitrary close to the spectral radius, i.e.\n",
    "$$\n",
    "\\rho(M) = \\sup_{ \\text{norms} \\| \\cdot \\| } \\| M \\|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-recording",
   "metadata": {},
   "source": [
    "Optimizing the relaxation parameter $\\alpha$\n",
    "---\n",
    "Let $A$ be SPD, and let $\\sigma(A) = \\{ \\lambda_i \\in R \\}$ with $0 < \\lambda_1 \\leq \\lambda_2 \\ldots \\leq \\lambda_n$.\n",
    "\n",
    "Then the eigenvalues of $M = I - \\alpha A$ are $\\{ 1 - \\alpha \\lambda_i  \\}$. \n",
    "\n",
    "Whenever we choose \n",
    "$$\n",
    "0 < \\alpha < \\frac{2}{\\lambda_n}\n",
    "$$\n",
    "we obtain $\\rho(M) < 1$ and a convergent iteration.\n",
    "\n",
    "\n",
    "The spectral radius of $M$ is \n",
    "\n",
    "$$\n",
    "\\rho(M) = \\max \\{ | 1 - \\alpha \\lambda_i| \\}  = \n",
    "\\max \\{ 1 - \\alpha \\lambda_1, - (1-\\alpha \\lambda_n) \\}\n",
    "$$\n",
    "\n",
    "The maximum is minimized if we choose $\\alpha$ optimally such that\n",
    "$$\n",
    "1 - \\alpha \\lambda_1 = - (1 - \\alpha \\lambda_n),\n",
    "$$\n",
    "i.e.\n",
    "$$\n",
    "\\alpha_{\\text{opt}} = \\frac{2}{\\lambda_1 + \\lambda_n} \n",
    "$$\n",
    "leading to the optimal convergence rate\n",
    "\n",
    "$$\n",
    "\\rho_{\\text{opt}} = \\frac{\\lambda_n - \\lambda_1}{\\lambda_n+\\lambda_1}\n",
    "\\approx 1 - 2 \\frac{\\lambda_1}{\\lambda_n} = 1 - \\frac{2}{\\kappa(A)}\n",
    "$$\n",
    "\n",
    "with the condition number $\\kappa(A) = \\lambda_n(A) / \\lambda_1(A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-sending",
   "metadata": {},
   "source": [
    "Experiments with the Richardson iteration\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-ecology",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngsolve import *\n",
    "from netgen.geom2d import unit_square\n",
    "mesh = Mesh(unit_square.GenerateMesh(maxh=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-services",
   "metadata": {},
   "outputs": [],
   "source": [
    "fes = H1(mesh, order=1)\n",
    "u,v = fes.TnT()\n",
    "a = BilinearForm(grad(u)*grad(v)*dx+10*u*v*dx).Assemble()\n",
    "f = LinearForm(x*y*v*dx).Assemble()\n",
    "gfu = GridFunction(fes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-indiana",
   "metadata": {},
   "source": [
    "we determine (an approximation to) the largest eigenvalue by a few steps of the power iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-remains",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv = gfu.vec.CreateVector()\n",
    "hv2 = gfu.vec.CreateVector()\n",
    "hv.SetRandom()\n",
    "hv.data /= Norm(hv)\n",
    "for k in range(100):\n",
    "    hv2.data = a.mat * hv\n",
    "    rho = Norm(hv2)\n",
    "    print (rho)\n",
    "    hv.data = 1/rho * hv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1 / rho\n",
    "res = f.vec.CreateVector()\n",
    "gfu.vec[:] = 0\n",
    "for k in range(1000):\n",
    "    res.data = f.vec - a.mat * gfu.vec\n",
    "    print (\"iteration\", k, \"res=\", Norm(res))\n",
    "    gfu.vec.data += alpha * res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngsolve.webgui import Draw\n",
    "Draw (gfu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-europe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
